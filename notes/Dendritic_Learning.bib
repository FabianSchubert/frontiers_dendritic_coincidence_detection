Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Law1994,
abstract = {The Bienenstock, Cooper, and Munro (BCM) theory of synaptic plasticity has successfully reproduced the development of orientation selectivity and ocular dominance in kitten visual cortex in normal, as well as deprived, visual environments. To better compare the consequences of this theory with experiment, previous abstractions of the visual environment are replaced in this work by real visual images with retinal processing. The visual environment is represented by 24 gray-scale natural images that are shifted across retinal fields. In this environment, the BCM neuron develops receptive fields similar to the fields of simple cells found in kitten striate cortex. These fields display adjacent excitatory and inhibitory bands when tested with spot stimuli, orientation selectivity when tested with bar stimuli, and spatial-frequency selectivity when tested with sinusoidal gratings. In addition, their development in various deprived visual environments agrees with experimental results.},
author = {Law, C. Charles and Cooper, Leon N.},
doi = {10.1073/pnas.91.16.7797},
file = {:home/fabian/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Cooper - 1994 - Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM.pdf:pdf},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {natural images,synaptic modification,visual cortex},
month = {aug},
number = {16},
pages = {7797--7801},
pmid = {8052662},
publisher = {National Academy of Sciences},
title = {{Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory}},
url = {/pmc/articles/PMC44489/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC44489/},
volume = {91},
year = {1994}
}
@article{Shai_2015,
author = {Shai, A S and Anastassiou, C A and Larkum, M E and Koch, C},
journal = {PLOS Computational Biology},
number = {3},
title = {{Physiology of Layer 5 Pyramidal Neurons in Mouse Primary Visual Cortex: Coincidence Detection through Bursting}},
volume = {11},
year = {2015}
}
@article{Intrator1992,
abstract = {In this paper, we present an objective function formulation of the Bienenstock, Cooper, and Munro (BCM) theory of visual cortical plasticity that permits us to demonstrate the connection between the unsupervised BCM learning procedure and various statistical methods, in particular, that of Projection Pursuit. This formulation provides a general method for stability analysis of the fixed points of the theory and enables us to analyze the behavior and the evolution of the network under various visual rearing conditions. It also allows comparison with many existing unsupervised methods. This model has been shown successful in various applications such as phoneme and 3D object recognition. We thus have the striking and possibly highly significant result that a biological neuron is performing a sophisticated statistical procedure. {\textcopyright} 1992 Pergamon Press plc.},
author = {Intrator, Nathan and Cooper, Leon N.},
doi = {10.1016/S0893-6080(05)80003-6},
issn = {08936080},
journal = {Neural Networks},
keywords = {Dimensionality reduction,Feature extraction,Unsupervised learning},
month = {jan},
number = {1},
pages = {3--17},
publisher = {Pergamon},
title = {{Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions}},
volume = {5},
year = {1992}
}
@article{Bienenstock1982,
abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents complete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
author = {Bienenstock, E. L. and Cooper, L. N. and Munro, P. W.},
doi = {10.1523/jneurosci.02-01-00032.1982},
issn = {02706474},
journal = {Journal of Neuroscience},
month = {jan},
number = {1},
pages = {32--48},
pmid = {7054394},
publisher = {Society for Neuroscience},
title = {{Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex}},
url = {https://www.jneurosci.org/content/2/1/32 https://www.jneurosci.org/content/2/1/32.abstract},
volume = {2},
year = {1982}
}
@article{Schiess_2016,
abstract = {In the last decade dendrites of cortical neurons have been shown to nonlinearly combine synaptic inputs by evoking local dendritic spikes. It has been suggested that these nonlinearities raise the computational power of a single neuron, making it comparable to a 2-layer network of point neurons. But how these nonlinearities can be incorporated into the synaptic plasticity to optimally support learning remains unclear. We present a theoretically derived synaptic plasticity rule for supervised and reinforcement learning that depends on the timing of the presynaptic, the dendritic and the postsynaptic spikes. For supervised learning, the rule can be seen as a biological version of the classical error-backpropagation algorithm applied to the dendritic case. When modulated by a delayed reward signal, the same plasticity is shown to maximize the expected reward in reinforcement learning for various coding scenarios. Our framework makes specific experimental predictions and highlights the unique advantage of active dendrites for implementing powerful synaptic plasticity rules that have access to downstream information via backpropagation of action potentials.},
author = {Schiess, Mathieu and Urbanczik, Robert and Senn, Walter},
doi = {10.1371/journal.pcbi.1004638},
file = {:home/fabian/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schiess, Urbanczik, Senn - 2016 - Somato-dendritic Synaptic Plasticity and Error-backpropagation in Active Dendrites.pdf:pdf},
issn = {15537358},
journal = {PLoS Computational Biology},
keywords = {Action potentials,Dendritic structure,Membrane potential,Neuronal dendrites,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity},
month = {feb},
number = {2},
pages = {1004638},
pmid = {26841235},
publisher = {Public Library of Science},
title = {{Somato-dendritic Synaptic Plasticity and Error-backpropagation in Active Dendrites}},
url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004638},
volume = {12},
year = {2016}
}
@article{Urbanczik_2014,
abstract = {Recent modeling of spike-timing-dependent plasticity indicates that plasticity involves as a third factor a local dendritic potential, besides pre- and postsynaptic firing times. We present a simple compartmental neuron model together with a non-Hebbian, biologically plausible learning rule for dendritic synapses where plasticity is modulated by these three factors. In functional terms, the rule seeks to minimize discrepancies between somatic firings and a local dendritic potential. Such prediction errors can arise in our model from stochastic fluctuations as well as from synaptic input, which directly targets the soma. Depending on the nature of this direct input, our plasticity rule subserves supervised or unsupervised learning. When a reward signal modulates the learning rate, reinforcement learning results. Hence a single plasticity rule supports diverse learning paradigms. {\textcopyright} 2014 Elsevier Inc.},
author = {Urbanczik, Robert and Senn, Walter},
doi = {10.1016/j.neuron.2013.11.030},
file = {:home/fabian/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Urbanczik, Senn - 2014 - Learning by the Dendritic Prediction of Somatic Spiking.pdf:pdf},
issn = {08966273},
journal = {Neuron},
month = {feb},
number = {3},
pages = {521--528},
pmid = {24507189},
publisher = {Elsevier},
title = {{Learning by the Dendritic Prediction of Somatic Spiking}},
url = {http://dx.},
volume = {81},
year = {2014}
}
@article{Guerguiev_2017,
abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi- compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher- order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations—the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.},
archivePrefix = {arXiv},
arxivId = {1610.00161},
author = {Guerguiev, Jordan and Lillicrap, Timothy P. and Richards, Blake A.},
doi = {10.7554/eLife.22901},
eprint = {1610.00161},
file = {:home/fabian/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guerguiev, Lillicrap, Richards - 2017 - Towards deep learning with segregated dendrites.pdf:pdf},
issn = {2050084X},
journal = {eLife},
month = {dec},
pmid = {29205151},
publisher = {eLife Sciences Publications Ltd},
title = {{Towards deep learning with segregated dendrites}},
volume = {6},
year = {2017}
}
